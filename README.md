# CS5260: Neural Networks II
## Assignment 6

## Student ID: A0248330L

Repository to experiment with Learning Rate Range Test across different optimizers and learning rate schedulers.

### Files

- A0248330L.ipynb (Modified Jupyter notebook)
- requirements.txt (Summarized with pip `freeze`)

### Experiments

- Optimizers
  1. Stochastic Gradient Descent (SGD)
  2. Adaptive Moment Estimation (ADAM)
  3. ADAMW
- Learning Rate Schedulers
  1. Lambda LR
  2. Exponential LR
  3. One Cycle LR
  4. No LR Scheduler

### How To Run

- Run the notebook sequentially through the steps
- In order to evaluate different pairs of optimizers and schedulers, change the value in the `Colossal AI Train` section
